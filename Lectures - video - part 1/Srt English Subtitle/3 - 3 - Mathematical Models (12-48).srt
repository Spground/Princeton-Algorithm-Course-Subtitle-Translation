1
00:00:02,079 --> 00:00:07,071
Observing what's happening as we did in
the last section it gives us a, a way to

2
00:00:07,071 --> 00:00:12,075
predict performance but it really doesn't
help us understand what the algorithm's

3
00:00:12,075 --> 00:00:17,056
doing. So next, we're going to look at
mathematical model. A way to get a better

4
00:00:17,056 --> 00:00:24,080
concept of what's really happening. Again,
this concept was really developed and

5
00:00:24,080 --> 00:00:33,047
popularized by Don Knuth starting in the
late 60s. At that time, computer systems

6
00:00:33,047 --> 00:00:40,068
were really becoming complicated for the
first time. And computer scientists were

7
00:00:40,068 --> 00:00:46,015
concerned about whether we really were
going to be able to understand what's

8
00:00:46,015 --> 00:00:52,043
going on. And Knuth was very direct in
saying that this is something that we

9
00:00:52,043 --> 00:00:56,884
certainly can do. We can calculate the
total running time of a program by

10
00:00:57,087 --> 00:01:02,059
identifying all the basic operations,
figuring out the cost, figuring out the

11
00:01:02,059 --> 00:01:07,043
frequency of execution and summing up the
cost times frequency for all the

12
00:01:07,043 --> 00:01:11,083
operations. You have to analyze the
program to determine what set of

13
00:01:11,083 --> 00:01:17,004
operations and the cost depends on the
machine and the computer in the system is

14
00:01:17,004 --> 00:01:22,038
what we talked about before. The frequency
leads us to mathematics because it depends

15
00:01:22,038 --> 00:01:27,369
on the algorithm and input data. Knuth has
written a series of books that give very

16
00:01:27,369 --> 00:01:34,678
detailed and all exact analyses within a
particular computer model for a wide range

17
00:01:34,678 --> 00:01:39,997
of algorithms. So, from Knuth, we know
that in principle, we can get accurate

18
00:01:39,997 --> 00:01:46,700
mathematical models for the performance of
algorithms or programs and operation. All

19
00:01:46,700 --> 00:01:53,156
right. So what, what does this process
look like? Well you can, if you want run

20
00:01:53,156 --> 00:01:59,044
experiments. In, in ancient times, we
would actually look at the computer manual

21
00:01:59,044 --> 00:02:04,621
and every computer came with a manual that
said precisely how long each instruction

22
00:02:04,621 --> 00:02:09,589
would take. But nowadays, it's a little
more complicated. So, we run experiments

23
00:02:09,589 --> 00:02:15,079
and, and you can go ahead and do a billion
ads and figure out that maybe on your

24
00:02:15,079 --> 00:02:20,432
computer, an ad takes 2.1 nano seconds. Or
you can do more complicated function s

25
00:02:20,432 --> 00:02:25,126
like computer sign or an arc tangent
although that's already getting close to

26
00:02:25,126 --> 00:02:30,644
the analysis of algorithms. So, there's
some way to determine the costs of the

27
00:02:30,644 --> 00:02:38,182
basic operations. And so, we'll just in
most, most of the cases we'll just

28
00:02:38,182 --> 00:02:44,137
postulate that it's some constant and you
can figure out what the constant is.

29
00:02:44,137 --> 00:02:49,477
Although when we're working with a
collection of objects, of anobjects there

30
00:02:49,477 --> 00:02:54,845
are some things that takes time
proportional to N like if you're going to

31
00:02:54,845 --> 00:03:00,591
allocate a array of size N it takes time
proportional to N because in Java the

32
00:03:00,591 --> 00:03:06,061
default is that all the elements in the
array initialize to zero. In other

33
00:03:06,061 --> 00:03:11,951
operations it depends on the system
implementation and an important one is

34
00:03:11,951 --> 00:03:17,503
string concatenation. If you concatenate
two strings the running time is

35
00:03:17,503 --> 00:03:23,832
proportional to the length of the string.
In many novices programming in Java, make

36
00:03:23,832 --> 00:03:30,713
a mistake of assuming that's a constant
time operation when its not. Alright, so

37
00:03:30,713 --> 00:03:39,428
that's the cost of each operation. More
interesting is the frequency of operation,

38
00:03:39,428 --> 00:03:46,229
of execution of the operations. So this is
a, a, it's a very simple variant of the

39
00:03:46,229 --> 00:03:50,237
three sum problem. That's the one sum
problem. That's how many numbers are

40
00:03:50,237 --> 00:03:54,333
actually equal to zero? How many single
numbers add up to zero? So, that one, it's

41
00:03:54,333 --> 00:03:59,111
just one four loop, and we go through, and
we tested the number zero and increment or

42
00:03:59,111 --> 00:04:05,417
count. And by analyzing that code you can
see that I and count have to be declared

43
00:04:05,417 --> 00:04:11,511
and then they have to be assigned to zero.
There's compares of i against N and

44
00:04:11,511 --> 00:04:17,484
there's N + one of them. There's compares
of A(i) against zero, there's N of those,

45
00:04:17,484 --> 00:04:23,985
N array axises and the number incremented
is number of times there's an increment is

46
00:04:23,985 --> 00:04:30,251
variable. I has incremented N times, but
count could be incremented any number from

47
00:04:30,251 --> 00:04:37,432
zero to N times. And so that frequency is
dependent on the input data. Or we might

48
00:04:37,432 --> 00:04:43,369
need a model for describing that or maybe
there's other operations that are more e

49
00:04:43,369 --> 00:04:48,289
xpensive and we won't need to worry about
that. So, let's look at the next more

50
00:04:48,289 --> 00:04:53,678
complicated problem is what about the
frequency of execution of instructions in

51
00:04:53,678 --> 00:04:59,039
this program which is the two sum problem,
how many pairs of integers sum to zero?

52
00:04:59,324 --> 00:05:06,207
Well, in this case, you have to do a
little bit of math to see that when we

53
00:05:06,465 --> 00:05:14,510
when i goes from zero to N, and j goes
from i + a to N the number of compares

54
00:05:14,510 --> 00:05:21,812
that we do work, plus array axises that we
do is two for each time the if statement

55
00:05:21,812 --> 00:05:28,044
is executed for Ai and Aj and that time
is, thing is executed N - one times the

56
00:05:28,044 --> 00:05:34,394
first time through the loop and N -two^2
and so forth. It's the sum of the integers

57
00:05:34,637 --> 00:05:40,395
from zero up to N - one which is a simple
discrete sum one-half N, (N - one) and

58
00:05:40,395 --> 00:05:47,383
since, and since we're doing it twice the
number of array axises is N, N - one. So,

59
00:05:47,383 --> 00:05:54,516
we can go ahead and get these actual exact
counts. But already, it's getting a little

60
00:05:54,516 --> 00:06:01,465
bit tedious to do that. And as far back as
Turing who also knew that and as well as

61
00:06:01,465 --> 00:06:06,921
Babbage did, that we want to have a
measure of the amount of work involved in

62
00:06:06,921 --> 00:06:12,909
the process. He recognized that you didn't
want to necessarily go through and do it

63
00:06:12,909 --> 00:06:18,975
in full detail. It's still helpful to have
a crude estimate. So, you could count up

64
00:06:18,975 --> 00:06:24,920
the number of times that every operation
is applied, give it weights and, and count

65
00:06:24,920 --> 00:06:32,113
the [inaudible] and so forth. But maybe we
should just count the ones that are most

66
00:06:32,113 --> 00:06:39,614
expensive that's what Turing said in 1947,
and realistically that's what we do

67
00:06:39,614 --> 00:06:46,852
nowadays. So rather than going in and
counting every little detail, we take some

68
00:06:46,852 --> 00:06:52,856
basic operation that's maybe the most
expensive and or and or the one that's

69
00:06:53,071 --> 00:06:59,480
executed the most often. The one that cost
and frequency is the highest and use that

70
00:06:59,480 --> 00:07:05,075
as a proxy for running time. Essentially,
making the hypothesis that the running

71
00:07:05,075 --> 00:07:10,237
time is, is going to grow like a constant
times [inaudible], So, in this case, were

72
00:07:10,237 --> 00:07:15,278
going to pick array axises. So, that's the
first simplification. And the second

73
00:07:15,278 --> 00:07:20,383
simplification is that we're going to
ignore low order terms in the formulas

74
00:07:20,383 --> 00:07:26,239
that we derive. And there's an easy way to
do that. It's called the tilde notation

75
00:07:26,481 --> 00:07:33,552
and, and the idea is when N is large in a
formula like this the N^3 term is much,

76
00:07:33,552 --> 00:07:40,492
much higher than the N term or sixteen. In
fact, so much so that we wouldn't even

77
00:07:40,773 --> 00:07:47,845
hardly notice these low order terms. So,
all of these formulas are tilde one-sixth

78
00:07:47,853 --> 00:07:54,344
N^3 and that's a fine representative or
approximate, approximation to these

79
00:07:54,344 --> 00:08:00,766
quantities. And it greatly simplifies
their calculations to for a, through a way

80
00:08:00,766 --> 00:08:05,963
to lower, lower to terms like this. So, by
focusing on one operation and , throwing

81
00:08:05,963 --> 00:08:11,653
away the tildes, the lower the terms and
this is the technical definition of tilde.

82
00:08:11,885 --> 00:08:16,868
It's just, F(N) tilde G (N) means the
limit as FN or GN equals one, and you can

83
00:08:16,868 --> 00:08:23,395
check that that's going to hold in these
kinds of situations. So, that greatly

84
00:08:23,395 --> 00:08:30,482
simplifies the frequency counts. And if
we're only picking one thing we're just

85
00:08:30,482 --> 00:08:36,049
talking about tilde N^2 and maybe another
tilde N^2 for the increment for the two

86
00:08:36,049 --> 00:08:41,323
sum problems, okay. So again, when N is
large, the terms are negligible and when N

87
00:08:41,323 --> 00:08:46,493
is really small, they're not negligible
but we don't really care because we're

88
00:08:46,493 --> 00:08:51,522
trying to estimate running times for large
N and running times for small N are going

89
00:08:51,522 --> 00:08:57,491
to be small no matter what. All right, so
now, we're using both the cost model and

90
00:08:57,491 --> 00:09:04,228
the tilde notation and then we can simply
say, that this program uses tilde N^2

91
00:09:04,228 --> 00:09:11,630
squared array axises and have implicit the
hypothesis that we think the running time

92
00:09:11,630 --> 00:09:18,362
is going to be tilde, a constant, times N
squared. Okay, we now what about three

93
00:09:18,362 --> 00:09:25,618
sums, let's do a, a real problem. So now,
we have the triple loop. And then, we have

94
00:09:25,618 --> 00:09:32,888
to do a more complicated combinatorial
problem in is not that big a deal really

95
00:09:33,151 --> 00:09:40,710
we are looking at the distinct number of
ways you can chose three things out of N

96
00:09:40,963 --> 00:09:48,188
and that 's binomial coefficient. And
again, doing the math and using the tilde,

97
00:09:48,188 --> 00:09:54,284
it's just tilde one-sixth N^3 three ray
axises for each triple so we can say

98
00:09:54,284 --> 00:10:01,006
one-half N^3. So we're not computing and
summing the costs of all operations that's

99
00:10:01,006 --> 00:10:07,581
too much work. We're picking the most
expensive in terms of cost times frequency

100
00:10:07,795 --> 00:10:15,170
and approximating that and trying to get a
good model for the running time. So now

101
00:10:15,462 --> 00:10:22,422
most, we're not going to do of a full
discrete mathematics in this course but

102
00:10:22,422 --> 00:10:29,101
there's some basic things that we'll want
to use and are, are not that difficult to

103
00:10:29,101 --> 00:10:35,245
understand. So, a lot of times we find out
that we need to come up with an estimate

104
00:10:35,245 --> 00:10:40,872
of a discrete sum. Like we did for one +
two up to N. Or some of the squares or

105
00:10:41,118 --> 00:10:48,044
other things like the three sum triple
loop. And so actually if you've had basic

106
00:10:48,044 --> 00:10:53,351
calculus, one way to think of it as to
just replace the sum with an interval,

107
00:10:53,351 --> 00:10:58,158
integral. That usually works or we can do
the math and use the so-called

108
00:10:58,158 --> 00:11:03,920
Eulerâ€“Maclaurin summation formula to get
a true approximation. But if you think of

109
00:11:03,920 --> 00:11:10,409
it this way you'll believe us when we say
that, that thing is tilde one-half N^2 or

110
00:11:10,409 --> 00:11:16,378
sum of one+ one-half + one-third up to one
/ N. That's like integral from x = one to

111
00:11:16,378 --> 00:11:22,262
N1 / x and that's natural log of N. Now
even the three sum triple loop kind of if

112
00:11:22,262 --> 00:11:27,214
you're used to multiple integrals, I will
quickly give you the one-sixth N^3.

113
00:11:27,438 --> 00:11:33,368
There's many more and other techniques
that we could use for this. And we're not

114
00:11:33,368 --> 00:11:38,774
going to teach all that, but we'll
sometimes refer to results of this type.

115
00:11:38,774 --> 00:11:45,637
Alright, so in principle, Knuth tells us
that accurate mathematical models are

116
00:11:45,637 --> 00:11:52,928
available in practice, we can get really
complicated formulas. We also might need

117
00:11:52,928 --> 00:11:59,568
some advance mathematics that the
theoretician will revel in. But that maybe

118
00:11:59,799 --> 00:12:06,565
people learning algorithms for the first
time might not be expected to know. So in

119
00:12:06,565 --> 00:12:12,338
the end careful exact models are best,
best left for exit, experts. There's

120
00:12:12,338 --> 00:12:19,079
really a lot of things that can go on. On
the other hand approximate models are

121
00:12:19,079 --> 00:12:25,087
definitely worthwhile. And for all the
algorithms that we consider we'll try to

122
00:12:25,087 --> 00:12:31,656
communicate a reasonable approximate model
that can be used to describe the running

123
00:12:31,656 --> 00:12:38,035
time. Sometimes we'll give the
mathematical proofs and other times we'll

124
00:12:38,035 --> 00:12:48,060
have to just cite the work of some expert.
